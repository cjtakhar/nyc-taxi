ğŸš– NYC Taxi Data Pipeline â€” Modern Data Stack (Airflow + Postgres + dbt)

This project implements a full modern data engineering pipeline using:

Apache Airflow â€“ workflow orchestration

Postgres â€“ data warehouse

dbt â€“ transformations & analytics layer

Docker Compose â€“ reproducible local environment

Python â€“ bulk ingestion (Parquet â†’ Postgres)

It ingests real NYC Yellow Taxi data (2023), loads it into a warehouse, transforms it using dbt, and prepares the data for analytics and dashboards.

Architecture

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Parquet     â”‚
          â”‚ NYC Taxi     â”‚
          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Airflow DAG    â”‚
        â”‚ bulk_load task   â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Warehouse (PG)  â”‚
     â”‚ raw_nyc_taxi...  â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    dbt         â”‚
      â”‚ staging/marts  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


ğŸ§© Components
1. Airflow

Runs inside Docker

Hosts the DAG: nyc_taxi_bulk_load_pipeline

Responsible for orchestrating data ingestion

2. Postgres Warehouse

Two Postgres services are defined:

airflow-postgres â†’ Airflow metadata

warehouse-postgres â†’ NYC taxi warehouse (nyc_taxi database)

3. dbt

Performs transformations (stg_taxi_trips model)

Creates a clean schema ready for analytics

ğŸš€ Features
âœ¨ Raw data ingestion (Parquet â†’ Postgres)

Using a Python script:

airflow/scripts/bulk_load_nyc_taxi.py

The script:

Reads a Parquet file

Subsets and orders columns

Creates a raw table

Performs a high-performance COPY into Postgres

âœ¨ Orchestration with Airflow

DAG located in:

airflow/dags/nyc_taxi_pipeline_dag.py


Runs the ingestion task:

bulk_load_parquet_to_postgres

âœ¨ dbt models

Located under:

dbt/


stg_taxi_trips.sql

schema.yml

dbt_project.yml

ğŸ› ï¸ Getting Started
Prerequisites

Docker Desktop installed

Python (optional, only needed if editing scripts locally)

ğŸ”§ Setup Instructions

Clone the repo and go to the root:

git clone <repo>
cd nyc-taxi


Ensure your folder structure looks like:

nyc-taxi/
  airflow/
    dags/
    scripts/
  data/
    yellow_tripdata_2023-01.parquet
  dbt/
  docker-compose.yml

â–¶ï¸ Start the Pipeline
1. Initialize Airflow
docker compose up airflow-init

2. Start Airflow Webserver + Scheduler
docker compose up -d airflow-webserver airflow-scheduler


Open the UI:

ğŸ‘‰ http://localhost:8080

Default login (if you created user manually):

admin / admin

ğŸ§ª Run the Pipeline

In the Airflow UI:

Turn on nyc_taxi_bulk_load_pipeline

Click Trigger DAG

Monitor the task in the Graph or Grid view.

ğŸ“Š Validate the Load

Connect to warehouse Postgres:

psql -h localhost -p 5433 -U nyc_taxi -d nyc_taxi


Password: nyc_taxi

Check the table:

SELECT COUNT(*) FROM raw_nyc_taxi_trips;


If rows appear â†’ the ingestion succeeded!

ğŸ§± dbt Transformations

Inside the Airflow webserver container:

docker exec -it airflow-webserver bash
cd /opt/airflow/dags/dbt/nyc_taxi
dbt run
dbt test


This builds:

staging models

marts (if added)

schema tests

ğŸ¯ Roadmap (optional next steps)

Load multiple months of data

Add a mart model (fact table)

Build dashboards in Looker Studio or Metabase

Add alerts (Slack/Email)

Add S3/GCS ingestion

Deploy on Airflow Cloud or MWAA

ğŸ“Œ Summary

This project demonstrates:

âœ” Data ingestion engineering
âœ” Workflow orchestration (Airflow)
âœ” Data modeling (dbt)
âœ” Warehouse design (Postgres RAW layer)
âœ” End-to-end MDS pipeline building
âœ” Docker-based reproducible environments
